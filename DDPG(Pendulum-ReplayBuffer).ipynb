{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen = 5000)\n",
    "        self.minibatch_size = 63\n",
    "\n",
    "    def append(self, state, action, reward, next_state, terminal):\n",
    "        self.buffer.append([state, action, reward, next_state, terminal])\n",
    "\n",
    "    def sample(self):\n",
    "        mini_batch = random.sample(self.buffer, self.minibatch_size)\n",
    "        mini_batch.append(self.buffer[-1])\n",
    "        s_lst, action, r_lst, s_prime_lst, done_mask_lst = map(list, zip(*mini_batch))\n",
    "        return torch.FloatTensor(s_lst).to(device), torch.FloatTensor(action).to(device), torch.FloatTensor(r_lst).to(device), \\\n",
    "                torch.FloatTensor(s_prime_lst).to(device), torch.FloatTensor(done_mask_lst).to(device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.actionNetwork = nn.Sequential(\n",
    "            nn.Linear(3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actionNetwork(state)\n",
    "    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.valueNetwork = nn.Sequential(\n",
    "            nn.Linear(3+1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        state = state.view(-1,state.shape[-1])\n",
    "        action = action.view(-1,action.shape[-1])\n",
    "        return self.valueNetwork(torch.cat([state, action], dim = -1))\n",
    "\n",
    "\n",
    "\n",
    "class DDPG():\n",
    "    def __init__(self):\n",
    "        super(DDPG, self).__init__()\n",
    "        self.actor = Actor()\n",
    "        self.actor_target = Actor()\n",
    "        self.critic = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.actionOptimizer = optim.Adam(self.actor.parameters(), lr = 0.001)\n",
    "        self.valueOptimizer = optim.Adam(self.critic.parameters(), lr = 0.001)\n",
    "        self.criticLoss = nn.MSELoss()\n",
    "        self.env = gym.make('Pendulum-v0')\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.tau = 0.001\n",
    "        self.gamma = 0.9\n",
    "        self.num_replay = 15\n",
    "        self.reward = 0\n",
    "        self.count = 0\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.flag = False\n",
    "        print(self.env.action_space.shape)\n",
    "        \n",
    "    def train(self, epi):\n",
    "        self.last_state = self.env.reset()\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.actor(torch.FloatTensor(self.last_state).view(-1,self.last_state.shape[-1]).to(device))\n",
    "            state, reward, done, _= self.env.step(action[0].detach().cpu())\n",
    "            self.count += 1\n",
    "            self.reward += reward\n",
    "            #print('action :',action)\n",
    "            self.replay_buffer.append(self.last_state, action.detach().cpu().numpy(), reward, state, done)\n",
    "            if self.replay_buffer.size()>self.replay_buffer.minibatch_size:\n",
    "                for _ in range(self.num_replay):\n",
    "                    self.optimize_network()\n",
    "\n",
    "            if done:\n",
    "                print(f'Epi : {epi}   Avg reward : {self.reward/self.count}')\n",
    "                break\n",
    "            self.last_state = state\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "            \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def optimize_network(self):\n",
    "        states, actions, rewards, next_states, terminals = self.replay_buffer.sample()\n",
    "        q_next_mat = self.critic_target(next_states, self.actor_target(next_states)).view(-1)\n",
    "        targetQ = rewards + q_next_mat*(1-terminals)*self.gamma\n",
    "    \n",
    "        self.valueOptimizer.zero_grad()\n",
    "        q_mat = self.critic(states, actions).view(-1)\n",
    "        valueLoss = self.criticLoss(q_mat,targetQ)\n",
    "        valueLoss.backward()\n",
    "        self.valueOptimizer.step()\n",
    "        \n",
    "        self.actionOptimizer.zero_grad()\n",
    "        q_mat = self.critic(states, self.actor(states)).view(-1)\n",
    "        actionLoss = (-q_mat.mean()).backward()\n",
    "        self.actionOptimizer.step()\n",
    "        \n",
    "        self.soft_update(self.critic, self.critic_target, self.tau)\n",
    "        self.soft_update(self.actor, self.actor_target, self.tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "Epi : 0   Avg reward : -6.7147722244262695\n",
      "Epi : 1   Avg reward : -7.488945484161377\n",
      "Epi : 2   Avg reward : -7.291668891906738\n",
      "Epi : 3   Avg reward : -7.558082103729248\n",
      "Epi : 4   Avg reward : -6.955118656158447\n",
      "Epi : 5   Avg reward : -6.620588302612305\n",
      "Epi : 6   Avg reward : -6.442498683929443\n",
      "Epi : 7   Avg reward : -6.172084331512451\n",
      "Epi : 8   Avg reward : -6.329494476318359\n",
      "Epi : 9   Avg reward : -6.149030685424805\n",
      "Epi : 10   Avg reward : -5.885885715484619\n",
      "Epi : 11   Avg reward : -5.731205463409424\n",
      "Epi : 12   Avg reward : -5.441232681274414\n",
      "Epi : 13   Avg reward : -5.197465419769287\n",
      "Epi : 14   Avg reward : -5.022012710571289\n",
      "Epi : 15   Avg reward : -4.7904767990112305\n",
      "Epi : 16   Avg reward : -4.548197269439697\n",
      "Epi : 17   Avg reward : -4.412535190582275\n",
      "Epi : 18   Avg reward : -4.251612186431885\n",
      "Epi : 19   Avg reward : -4.073489189147949\n",
      "Epi : 20   Avg reward : -3.9126720428466797\n",
      "Epi : 21   Avg reward : -3.8602805137634277\n",
      "Epi : 22   Avg reward : -3.81107759475708\n",
      "Epi : 23   Avg reward : -3.735250234603882\n",
      "Epi : 24   Avg reward : -3.6135292053222656\n",
      "Epi : 25   Avg reward : -3.524353504180908\n",
      "Epi : 26   Avg reward : -3.3955953121185303\n",
      "Epi : 27   Avg reward : -3.3854243755340576\n",
      "Epi : 28   Avg reward : -3.3163678646087646\n",
      "Epi : 29   Avg reward : -3.305405855178833\n",
      "Epi : 30   Avg reward : -3.2886996269226074\n",
      "Epi : 31   Avg reward : -3.2086009979248047\n",
      "Epi : 32   Avg reward : -3.1300482749938965\n",
      "Epi : 33   Avg reward : -3.0399997234344482\n",
      "Epi : 34   Avg reward : -3.00870680809021\n",
      "Epi : 35   Avg reward : -3.0181329250335693\n",
      "Epi : 36   Avg reward : -2.954802989959717\n",
      "Epi : 37   Avg reward : -2.9116289615631104\n",
      "Epi : 38   Avg reward : -2.887686014175415\n",
      "Epi : 39   Avg reward : -2.8321127891540527\n",
      "Epi : 40   Avg reward : -2.7796542644500732\n",
      "Epi : 41   Avg reward : -2.729234218597412\n",
      "Epi : 42   Avg reward : -2.6818737983703613\n",
      "Epi : 43   Avg reward : -2.6640801429748535\n",
      "Epi : 44   Avg reward : -2.605013847351074\n",
      "Epi : 45   Avg reward : -2.5488786697387695\n",
      "Epi : 46   Avg reward : -2.4951696395874023\n",
      "Epi : 47   Avg reward : -2.4433841705322266\n",
      "Epi : 48   Avg reward : -2.421463966369629\n",
      "Epi : 49   Avg reward : -2.3873748779296875\n",
      "Epi : 50   Avg reward : -2.368295669555664\n",
      "Epi : 51   Avg reward : -2.3607187271118164\n",
      "Epi : 52   Avg reward : -2.3284716606140137\n",
      "Epi : 53   Avg reward : -2.3193020820617676\n",
      "Epi : 54   Avg reward : -2.28829288482666\n",
      "Epi : 55   Avg reward : -2.2589828968048096\n",
      "Epi : 56   Avg reward : -2.230550527572632\n",
      "Epi : 57   Avg reward : -2.214891195297241\n",
      "Epi : 58   Avg reward : -2.197869062423706\n",
      "Epi : 59   Avg reward : -2.1722476482391357\n",
      "Epi : 60   Avg reward : -2.1471028327941895\n",
      "Epi : 61   Avg reward : -2.133479356765747\n",
      "Epi : 62   Avg reward : -2.110191583633423\n",
      "Epi : 63   Avg reward : -2.0981245040893555\n",
      "Epi : 64   Avg reward : -2.0760886669158936\n",
      "Epi : 65   Avg reward : -2.0545713901519775\n",
      "Epi : 66   Avg reward : -2.048985242843628\n",
      "Epi : 67   Avg reward : -2.037362813949585\n",
      "Epi : 68   Avg reward : -2.0270471572875977\n",
      "Epi : 69   Avg reward : -2.007591724395752\n",
      "Epi : 70   Avg reward : -1.9886969327926636\n",
      "Epi : 71   Avg reward : -1.979178786277771\n",
      "Epi : 72   Avg reward : -1.9709258079528809\n",
      "Epi : 73   Avg reward : -1.9712165594100952\n",
      "Epi : 74   Avg reward : -1.9449641704559326\n",
      "Epi : 75   Avg reward : -1.9280191659927368\n",
      "Epi : 76   Avg reward : -1.9215905666351318\n",
      "Epi : 77   Avg reward : -1.905531406402588\n",
      "Epi : 78   Avg reward : -1.8989694118499756\n",
      "Epi : 79   Avg reward : -1.9125256538391113\n",
      "Epi : 80   Avg reward : -1.8970164060592651\n",
      "Epi : 81   Avg reward : -1.8952382802963257\n",
      "Epi : 82   Avg reward : -1.8804353475570679\n",
      "Epi : 83   Avg reward : -1.8581547737121582\n",
      "Epi : 84   Avg reward : -1.8524292707443237\n",
      "Epi : 85   Avg reward : -1.8387699127197266\n",
      "Epi : 86   Avg reward : -1.824998140335083\n",
      "Epi : 87   Avg reward : -1.8116436004638672\n",
      "Epi : 88   Avg reward : -1.8048478364944458\n",
      "Epi : 89   Avg reward : -1.7921775579452515\n",
      "Epi : 90   Avg reward : -1.7798290252685547\n",
      "Epi : 91   Avg reward : -1.7676196098327637\n",
      "Epi : 92   Avg reward : -1.7555593252182007\n",
      "Epi : 93   Avg reward : -1.7436444759368896\n",
      "Epi : 94   Avg reward : -1.7325036525726318\n",
      "Epi : 95   Avg reward : -1.7144933938980103\n",
      "Epi : 96   Avg reward : -1.696864366531372\n",
      "Epi : 97   Avg reward : -1.6923733949661255\n",
      "Epi : 98   Avg reward : -1.688536286354065\n",
      "Epi : 99   Avg reward : -1.6846569776535034\n",
      "Epi : 100   Avg reward : -1.6803137063980103\n",
      "Epi : 101   Avg reward : -1.6699910163879395\n",
      "Epi : 102   Avg reward : -1.6603704690933228\n",
      "Epi : 103   Avg reward : -1.6506171226501465\n",
      "Epi : 104   Avg reward : -1.641682505607605\n",
      "Epi : 105   Avg reward : -1.6379523277282715\n",
      "Epi : 106   Avg reward : -1.629058837890625\n",
      "Epi : 107   Avg reward : -1.6324304342269897\n",
      "Epi : 108   Avg reward : -1.6175202131271362\n",
      "Epi : 109   Avg reward : -1.6142922639846802\n",
      "Epi : 110   Avg reward : -1.5997897386550903\n",
      "Epi : 111   Avg reward : -1.602001428604126\n",
      "Epi : 112   Avg reward : -1.5936543941497803\n",
      "Epi : 113   Avg reward : -1.5980862379074097\n",
      "Epi : 114   Avg reward : -1.5964840650558472\n",
      "Epi : 115   Avg reward : -1.5884230136871338\n",
      "Epi : 116   Avg reward : -1.5806175470352173\n",
      "Epi : 117   Avg reward : -1.5728909969329834\n",
      "Epi : 118   Avg reward : -1.565172791481018\n",
      "Epi : 119   Avg reward : -1.5632492303848267\n",
      "Epi : 120   Avg reward : -1.5700055360794067\n",
      "Epi : 121   Avg reward : -1.5630971193313599\n",
      "Epi : 122   Avg reward : -1.5611600875854492\n",
      "Epi : 123   Avg reward : -1.5588182210922241\n",
      "Epi : 124   Avg reward : -1.5619828701019287\n",
      "Epi : 125   Avg reward : -1.549652338027954\n",
      "Epi : 126   Avg reward : -1.5427056550979614\n",
      "Epi : 127   Avg reward : -1.5410887002944946\n",
      "Epi : 128   Avg reward : -1.542771816253662\n",
      "Epi : 129   Avg reward : -1.5411514043807983\n",
      "Epi : 130   Avg reward : -1.5447185039520264\n",
      "Epi : 131   Avg reward : -1.5378532409667969\n",
      "Epi : 132   Avg reward : -1.5263488292694092\n",
      "Epi : 133   Avg reward : -1.550779938697815\n",
      "Epi : 134   Avg reward : -1.569074034690857\n",
      "Epi : 135   Avg reward : -1.6124118566513062\n",
      "Epi : 136   Avg reward : -1.6240051984786987\n",
      "Epi : 137   Avg reward : -1.634263515472412\n",
      "Epi : 138   Avg reward : -1.6364556550979614\n",
      "Epi : 139   Avg reward : -1.6249200105667114\n",
      "Epi : 140   Avg reward : -1.618187427520752\n",
      "Epi : 141   Avg reward : -1.6598427295684814\n",
      "Epi : 142   Avg reward : -1.652946949005127\n",
      "Epi : 143   Avg reward : -1.6420012712478638\n",
      "Epi : 144   Avg reward : -1.6353081464767456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7ec2b78897a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-37ccf8599cb1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epi)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_replay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-37ccf8599cb1>\u001b[0m in \u001b[0;36moptimize_network\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mq_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[0mactionLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mq_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-37ccf8599cb1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1672\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1673\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1674\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = DDPG()\n",
    "\n",
    "for epi in range(200):\n",
    "    model.train(epi)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epi : 0   Avg reward : -7.331660270690918\n",
    "Epi : 1   Avg reward : -6.766748428344727\n",
    "Epi : 2   Avg reward : -7.019373416900635\n",
    "Epi : 3   Avg reward : -7.0845561027526855\n",
    "Epi : 4   Avg reward : -7.081833362579346\n",
    "Epi : 5   Avg reward : -7.101948261260986\n",
    "Epi : 6   Avg reward : -6.921932220458984\n",
    "Epi : 7   Avg reward : -6.668462753295898\n",
    "Epi : 8   Avg reward : -6.543397903442383\n",
    "Epi : 9   Avg reward : -6.431257724761963\n",
    "Epi : 10   Avg reward : -6.147215843200684\n",
    "Epi : 11   Avg reward : -5.742638111114502\n",
    "Epi : 12   Avg reward : -5.726519584655762\n",
    "Epi : 13   Avg reward : -5.541481971740723\n",
    "Epi : 14   Avg reward : -5.26165246963501\n",
    "Epi : 15   Avg reward : -5.021037578582764\n",
    "Epi : 16   Avg reward : -4.803654193878174\n",
    "Epi : 17   Avg reward : -4.693840026855469\n",
    "Epi : 18   Avg reward : -4.8417816162109375\n",
    "Epi : 19   Avg reward : -4.664757251739502\n",
    "Epi : 20   Avg reward : -4.503854274749756\n",
    "Epi : 21   Avg reward : -4.567991256713867\n",
    "Epi : 22   Avg reward : -4.39901065826416\n",
    "Epi : 23   Avg reward : -4.245830059051514\n",
    "Epi : 24   Avg reward : -4.188897132873535\n",
    "Epi : 25   Avg reward : -4.07916259765625\n",
    "Epi : 26   Avg reward : -4.208540916442871\n",
    "Epi : 27   Avg reward : -4.083531379699707\n",
    "Epi : 28   Avg reward : -3.966477632522583\n",
    "Epi : 29   Avg reward : -3.856781005859375\n",
    "Epi : 30   Avg reward : -3.7535760402679443\n",
    "Epi : 31   Avg reward : -3.6942529678344727\n",
    "Epi : 32   Avg reward : -3.6025590896606445\n",
    "Epi : 33   Avg reward : -3.515573024749756\n",
    "Epi : 34   Avg reward : -3.4332034587860107\n",
    "Epi : 35   Avg reward : -3.374929904937744\n",
    "Epi : 36   Avg reward : -3.3011815547943115\n",
    "Epi : 37   Avg reward : -3.2485601902008057\n",
    "Epi : 38   Avg reward : -3.226588249206543\n",
    "Epi : 39   Avg reward : -3.162376642227173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'C:\\\\ProgramStudy\\\\RL_Drone\\\\AirSim\\\\PythonClient\\\\multirotor\\\\parameter.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
